"""
Multi-Model AI Financial Writer Pro - Streamlit Cloud Version
Supports Anthropic Claude, OpenAI GPT, and Google Gemini
"""

import streamlit as st
from typing import Dict, Any
import json
from datetime import datetime
from pathlib import Path
import time

# Try cloud config first, fallback to local
try:
    from src.config_cloud import config
except:
    from src.config import config

from src.agents.multi_model_agents import (
    ModelFactory,
    MultiModelAgent,
    MultiModelAgentResponse
)
from src.agents.loop_agent import LoopAgent
from src.tools.custom_tools import (
    validate_financial_terms,
    check_compliance,
    calculate_quality_score
)

# Configure page
st.set_page_config(
    page_title="AI Financial Writer Pro - Multi Model",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .stApp {
        font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
    }
    
    .main-header {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 2rem;
        border-radius: 16px;
        margin-bottom: 2rem;
        color: white;
    }
    
    .model-card {
        background: white;
        border-radius: 12px;
        padding: 1rem;
        border: 2px solid #e2e8f0;
        margin-bottom: 1rem;
        transition: all 0.3s ease;
    }
    
    .model-card:hover {
        border-color: #667eea;
        box-shadow: 0 4px 12px rgba(102, 126, 234, 0.15);
    }
    
    .provider-badge {
        display: inline-block;
        padding: 0.25rem 0.75rem;
        border-radius: 20px;
        font-size: 0.875rem;
        font-weight: 600;
        margin-right: 0.5rem;
    }
    
    .anthropic-badge {
        background: linear-gradient(135deg, #FF6B6B 0%, #4ECDC4 100%);
        color: white;
    }
    
    .openai-badge {
        background: linear-gradient(135deg, #74b9ff 0%, #0984e3 100%);
        color: white;
    }
    
    .google-badge {
        background: linear-gradient(135deg, #4285f4 0%, #34a853 100%);
        color: white;
    }
    
    .stat-card {
        background: white;
        border-radius: 12px;
        padding: 1.5rem;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        margin-bottom: 1rem;
    }
    
    .metric-value {
        font-size: 2rem;
        font-weight: 700;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
    }
    
    .comparison-panel {
        background: white;
        border-radius: 12px;
        padding: 1.5rem;
        border: 2px solid #e2e8f0;
        margin-bottom: 1rem;
    }
    
    .stButton > button {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        border: none;
        padding: 0.75rem 2rem;
        border-radius: 12px;
        font-weight: 600;
        width: 100%;
    }
    
    .stButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 0 10px 20px rgba(102, 126, 234, 0.3);
    }
</style>
""", unsafe_allow_html=True)

class MultiModelFinancialWritingApp:
    """Multi-Model Financial Writing AI Application"""
    
    def __init__(self):
        self.config = config
        self.multi_model_agent = None
        self._initialize_session_state()
    
    def _initialize_session_state(self):
        """Initialize session state variables"""
        if 'history' not in st.session_state:
            st.session_state.history = []
        if 'current_result' not in st.session_state:
            st.session_state.current_result = None
        if 'stats' not in st.session_state:
            st.session_state.stats = {
                'total_documents': 0,
                'avg_quality': 0,
                'avg_iterations': 0,
                'total_time': 0,
                'models_used': {}
            }
        if 'selected_provider' not in st.session_state:
            st.session_state.selected_provider = config.DEFAULT_PROVIDER
        if 'selected_model' not in st.session_state:
            st.session_state.selected_model = None
    
    def render_header(self):
        """Render header"""
        st.markdown("""
        <div class="main-header">
            <h1 style="margin: 0; font-size: 2.5rem;">ü§ñ AI Financial Writer Pro - Multi Model</h1>
            <p style="margin-top: 0.5rem; opacity: 0.9;">
                Ï∞®ÏÑ∏ÎåÄ Í∏àÏúµ Î¨∏ÏÑú ÏûëÏÑ± AI | Anthropic Claude ‚Ä¢ OpenAI GPT ‚Ä¢ Google Gemini
            </p>
        </div>
        """, unsafe_allow_html=True)
    
    def render_model_selector(self):
        """Render model selection UI"""
        st.markdown("## üéØ AI Î™®Îç∏ ÏÑ†ÌÉù")
        
        # Get available models
        available_models = ModelFactory.get_available_models()
        
        # Provider selection
        col1, col2, col3 = st.columns(3)
        
        providers_available = []
        
        with col1:
            if config.ANTHROPIC_API_KEY:
                providers_available.append("Anthropic")
                if st.button("üî¥ Anthropic Claude", type="secondary" if st.session_state.selected_provider != "Anthropic" else "primary"):
                    st.session_state.selected_provider = "Anthropic"
                    st.rerun()
        
        with col2:
            if config.OPENAI_API_KEY:
                providers_available.append("OpenAI")
                if st.button("üîµ OpenAI GPT", type="secondary" if st.session_state.selected_provider != "OpenAI" else "primary"):
                    st.session_state.selected_provider = "OpenAI"
                    st.rerun()
        
        with col3:
            if config.GOOGLE_API_KEY:
                providers_available.append("Google")
                if st.button("üü¢ Google Gemini", type="secondary" if st.session_state.selected_provider != "Google" else "primary"):
                    st.session_state.selected_provider = "Google"
                    st.rerun()
        
        # Model selection for current provider
        if st.session_state.selected_provider and st.session_state.selected_provider in available_models:
            st.markdown(f"### Î™®Îç∏ ÏÑ†ÌÉù: {st.session_state.selected_provider}")
            
            models = available_models[st.session_state.selected_provider]
            
            # Create model cards
            cols = st.columns(2)
            for idx, (model_id, model_name) in enumerate(models.items()):
                with cols[idx % 2]:
                    if st.button(
                        f"{model_name}",
                        key=f"model_{model_id}",
                        help=f"Î™®Îç∏ ID: {model_id}"
                    ):
                        st.session_state.selected_model = model_id
                        st.success(f"‚úÖ {model_name} ÏÑ†ÌÉùÎê®")
            
            # Show current selection
            if st.session_state.selected_model:
                st.info(f"ÌòÑÏû¨ ÏÑ†ÌÉùÎêú Î™®Îç∏: **{models.get(st.session_state.selected_model, st.session_state.selected_model)}**")
        
        return providers_available
    
    def render_stats(self):
        """Render statistics"""
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric(
                "Ï¥ù ÏÉùÏÑ± Î¨∏ÏÑú",
                st.session_state.stats['total_documents'],
                "+1" if st.session_state.stats['total_documents'] > 0 else None
            )
        
        with col2:
            st.metric(
                "ÌèâÍ∑† ÌíàÏßà",
                f"{st.session_state.stats['avg_quality']:.1%}",
                f"+{(st.session_state.stats['avg_quality'] - 0.7) * 100:.0f}%" if st.session_state.stats['avg_quality'] > 0 else None
            )
        
        with col3:
            st.metric(
                "ÌèâÍ∑† Î∞òÎ≥µ",
                f"{st.session_state.stats['avg_iterations']:.1f}Ìöå",
                None
            )
        
        with col4:
            # Show most used model
            if st.session_state.stats['models_used']:
                most_used = max(st.session_state.stats['models_used'].items(), key=lambda x: x[1])
                st.metric(
                    "Ï£ºÎ°ú ÏÇ¨Ïö©Îêú Î™®Îç∏",
                    most_used[0],
                    f"{most_used[1]}Ìöå ÏÇ¨Ïö©"
                )
            else:
                st.metric("Ï£ºÎ°ú ÏÇ¨Ïö©Îêú Î™®Îç∏", "-", None)
    
    def process_document(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Process document through the pipeline"""
        try:
            # Initialize multi-model agent if not already done
            if not self.multi_model_agent:
                agent_config = config.get_agent_config()
                self.multi_model_agent = MultiModelAgent(agent_config)
            
            # Show progress
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # Get selected provider and model
            provider = st.session_state.selected_provider
            model = st.session_state.selected_model
            
            if model:
                # Update config with selected model
                if provider == "Anthropic":
                    self.multi_model_agent.config['anthropic_model'] = model
                elif provider == "OpenAI":
                    self.multi_model_agent.config['openai_model'] = model
                elif provider == "Google":
                    self.multi_model_agent.config['google_model'] = model
            
            # Generate document using multi-model agent
            status_text.text(f"Î™®Îç∏ Ï§ÄÎπÑ Ï§ë... ({provider})")
            progress_bar.progress(20)
            
            # Create prompt for document generation
            prompt = self._create_prompt(input_data)
            
            status_text.text(f"Î¨∏ÏÑú ÏÉùÏÑ± Ï§ë... ({provider})")
            progress_bar.progress(50)
            
            # Generate response
            response = self.multi_model_agent.generate(
                prompt,
                provider=provider,
                temperature=input_data.get('temperature', 0.7),
                max_tokens=input_data.get('max_tokens', 2048)
            )
            
            progress_bar.progress(80)
            status_text.text("ÌíàÏßà Í≤ÄÏ¶ù Ï§ë...")
            
            # Validation
            final_doc = response.content
            term_validation = validate_financial_terms(final_doc)
            compliance_check = check_compliance(final_doc, input_data.get("document_type", "email"))
            final_score = calculate_quality_score(final_doc, term_validation, compliance_check)
            
            progress_bar.progress(100)
            status_text.text("ÏôÑÎ£å!")
            
            # Update model usage stats
            model_key = f"{provider} - {response.model_used}"
            if model_key not in st.session_state.stats['models_used']:
                st.session_state.stats['models_used'][model_key] = 0
            st.session_state.stats['models_used'][model_key] += 1
            
            result = {
                "success": True,
                "final_document": final_doc,
                "quality_score": final_score,
                "iterations": 1,
                "total_time": 5.0,
                "provider": provider,
                "model_used": response.model_used,
                "validation": {
                    "terms": term_validation,
                    "compliance": compliance_check,
                    "final_score": final_score
                }
            }
            
            # Update stats
            self._update_stats(result)
            
            return result
            
        except Exception as e:
            return {"error": str(e), "success": False}
    
    def _create_prompt(self, input_data: Dict[str, Any]) -> str:
        """Create prompt for document generation"""
        doc_type = config.DOCUMENT_TYPES.get(input_data['document_type'], input_data['document_type'])
        
        prompt = f"""ÎãπÏã†ÏùÄ Ï†ÑÎ¨∏Ï†ÅÏù∏ Í∏àÏúµ Î¨∏ÏÑú ÏûëÏÑ± AIÏûÖÎãàÎã§.
        
Îã§Ïùå ÏöîÍµ¨ÏÇ¨Ìï≠Ïóê ÎßûÎäî {doc_type}ÏùÑ(Î•º) ÏûëÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî:

ÏöîÍµ¨ÏÇ¨Ìï≠: {input_data['requirements']}
ÌÜ§Ïï§Îß§ÎÑà: {input_data['tone']}
"""
        
        if input_data.get('recipient'):
            prompt += f"\nÏàòÏã†Ïûê: {input_data['recipient']}"
        
        if input_data.get('subject'):
            prompt += f"\nÏ†úÎ™©: {input_data['subject']}"
        
        if input_data.get('additional_context'):
            prompt += f"\nÏ∂îÍ∞Ä Ïª®ÌÖçÏä§Ìä∏: {input_data['additional_context']}"
        
        prompt += """

Îã§Ïùå Í∏∞Ï§ÄÏùÑ Ï∂©Ï°±ÌïòÎäî Ï†ÑÎ¨∏Ï†ÅÏù¥Í≥† Ï†ïÌôïÌïú Î¨∏ÏÑúÎ•º ÏûëÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî:
1. Í∏àÏúµ Ï†ÑÎ¨∏ Ïö©Ïñ¥Î•º Ï†ïÌôïÌïòÍ≤å ÏÇ¨Ïö©
2. Í∑úÏ†ï Ï§ÄÏàò Î∞è Î≤ïÏ†Å ÏöîÍµ¨ÏÇ¨Ìï≠ Ï∂©Ï°±
3. Î™ÖÌôïÌïòÍ≥† Í∞ÑÍ≤∞Ìïú Î¨∏Ïû• Íµ¨ÏÑ±
4. Ï†ÅÏ†àÌïú Íµ¨Ï°∞ÏôÄ ÌòïÏãù
5. Ï†ÑÎ¨∏Ï†ÅÏù¥Î©¥ÏÑúÎèÑ Ïù¥Ìï¥ÌïòÍ∏∞ Ïâ¨Ïö¥ ÌëúÌòÑ
"""
        
        return prompt
    
    def _update_stats(self, result: Dict[str, Any]):
        """Update statistics"""
        stats = st.session_state.stats
        stats['total_documents'] += 1
        
        current_quality = result.get('quality_score', 0)
        stats['avg_quality'] = (
            (stats['avg_quality'] * (stats['total_documents'] - 1) + current_quality) 
            / stats['total_documents']
        )
        
        current_iter = result.get('iterations', 0)
        stats['avg_iterations'] = (
            (stats['avg_iterations'] * (stats['total_documents'] - 1) + current_iter)
            / stats['total_documents']
        )
        
        stats['total_time'] += result.get('total_time', 0)
    
    def run(self):
        """Run the application"""
        # Header
        self.render_header()
        
        # Model selector
        providers_available = self.render_model_selector()
        
        if not providers_available:
            st.error("‚ö†Ô∏è API ÌÇ§Í∞Ä ÏÑ§Ï†ïÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§. Streamlit secrets ÎòêÎäî ÌôòÍ≤Ω Î≥ÄÏàòÎ•º ÌôïÏù∏Ìï¥Ï£ºÏÑ∏Ïöî.")
            st.stop()
        
        st.divider()
        
        # Stats
        self.render_stats()
        
        # Sidebar
        with st.sidebar:
            st.markdown("## ‚öôÔ∏è ÏÑ§Ï†ï")
            
            # Model settings
            st.markdown("### ü§ñ Î™®Îç∏ ÏÑ§Ï†ï")
            
            temperature = st.slider(
                "Temperature (Ï∞ΩÏùòÏÑ±)",
                min_value=0.0,
                max_value=1.0,
                value=config.TEMPERATURE,
                step=0.1,
                help="ÎÇÆÏùÑÏàòÎ°ù ÏùºÍ¥ÄÏÑ± ÏûàÍ≥†, ÎÜíÏùÑÏàòÎ°ù Ï∞ΩÏùòÏ†ÅÏûÖÎãàÎã§"
            )
            
            max_tokens = st.number_input(
                "ÏµúÎåÄ ÌÜ†ÌÅ∞ Ïàò",
                min_value=500,
                max_value=4000,
                value=config.MAX_OUTPUT_TOKENS,
                step=100
            )
            
            st.divider()
            
            # Document settings
            st.markdown("### üìÑ Î¨∏ÏÑú ÏÑ§Ï†ï")
            
            doc_type = st.selectbox(
                "Î¨∏ÏÑú Ïú†Ìòï",
                options=list(config.DOCUMENT_TYPES.keys()),
                format_func=lambda x: config.DOCUMENT_TYPES[x]
            )
            
            tone = st.select_slider(
                "ÌÜ§Ïï§Îß§ÎÑà",
                options=["formal", "professional", "professional_friendly", "friendly"],
                value="professional",
                format_func=lambda x: {
                    "formal": "Í≤©ÏãùÏûàÎäî",
                    "professional": "Ï†ÑÎ¨∏Ï†ÅÏù∏",
                    "professional_friendly": "Ï†ÑÎ¨∏Ï†ÅÏù¥Î©¥ÏÑú ÏπúÍ∑ºÌïú",
                    "friendly": "ÏπúÍ∑ºÌïú"
                }.get(x, x)
            )
            
            st.divider()
            
            with st.expander("üé® Î™®Îç∏Î≥Ñ ÌäπÏßï"):
                st.markdown("""
                **Anthropic Claude**
                - Í∏¥ Ïª®ÌÖçÏä§Ìä∏ Ï≤òÎ¶¨ Ïö∞Ïàò
                - ÎÖºÎ¶¨Ï†ÅÏù¥Í≥† Ï≤¥Í≥ÑÏ†ÅÏù∏ ÏûëÏÑ±
                - Î≥µÏû°Ìïú Î∂ÑÏÑùÏóê Í∞ïÏ†ê
                
                **OpenAI GPT**
                - Î≤îÏö©ÏÑ±Í≥º Í∑†ÌòïÏû°Ìûå ÏÑ±Îä•
                - Ï∞ΩÏùòÏ†ÅÏù∏ ÏûëÏÑ±Ïóê Í∞ïÏ†ê
                - Îã§ÏñëÌïú Ïä§ÌÉÄÏùº ÏßÄÏõê
                
                **Google Gemini**
                - Îπ†Î•∏ ÏùëÎãµ ÏÜçÎèÑ
                - Ìö®Ïú®Ï†ÅÏù∏ Ï≤òÎ¶¨
                - Î©ÄÌã∞Î™®Îã¨ ÏßÄÏõê
                """)
            
            st.divider()
            
            if st.button("üîÑ Ï¥àÍ∏∞Ìôî"):
                st.session_state.history = []
                st.session_state.current_result = None
                st.rerun()
        
        # Main content
        tab1, tab2, tab3, tab4 = st.tabs(["‚úçÔ∏è Î¨∏ÏÑú ÏûëÏÑ±", "üîç Î™®Îç∏ ÎπÑÍµê", "üìä ÎπÑÍµê Î∂ÑÏÑù", "üìö Ïù¥Î†•"])
        
        with tab1:
            col1, col2 = st.columns([1, 1])
            
            with col1:
                st.markdown("### üìù Î¨∏ÏÑú ÏöîÍµ¨ÏÇ¨Ìï≠")
                
                requirements = st.text_area(
                    "ÏöîÍµ¨ÏÇ¨Ìï≠",
                    placeholder="ÏûëÏÑ±ÌïòÍ≥†Ïûê ÌïòÎäî Î¨∏ÏÑúÏùò ÎÇ¥Ïö©Í≥º ÏöîÍµ¨ÏÇ¨Ìï≠ÏùÑ ÏûÖÎ†•ÌïòÏÑ∏Ïöî...",
                    height=200,
                    label_visibility="collapsed"
                )
                
                with st.expander("Ï∂îÍ∞Ä Ï†ïÎ≥¥"):
                    recipient = st.text_input("ÏàòÏã†Ïûê")
                    subject = st.text_input("Ï†úÎ™©")
                    additional_context = st.text_area("Ï∂îÍ∞Ä Ïª®ÌÖçÏä§Ìä∏", height=100)
                
                if st.button("üöÄ Î¨∏ÏÑú ÏÉùÏÑ±", type="primary"):
                    if requirements:
                        if not st.session_state.selected_model:
                            st.warning("Î®ºÏ†Ä AI Î™®Îç∏ÏùÑ ÏÑ†ÌÉùÌï¥Ï£ºÏÑ∏Ïöî.")
                        else:
                            input_data = {
                                "document_type": doc_type,
                                "requirements": requirements,
                                "tone": tone,
                                "recipient": recipient,
                                "subject": subject,
                                "additional_context": additional_context,
                                "temperature": temperature,
                                "max_tokens": max_tokens
                            }
                            
                            with st.spinner(f"{st.session_state.selected_provider} AIÍ∞Ä Î¨∏ÏÑúÎ•º ÏûëÏÑ± Ï§ëÏûÖÎãàÎã§..."):
                                result = self.process_document(input_data)
                            
                            st.session_state.current_result = result
                            st.session_state.history.append({
                                "timestamp": datetime.now().isoformat(),
                                "input": input_data,
                                "result": result
                            })
                            
                            if result.get("success"):
                                st.success(f"‚úÖ Î¨∏ÏÑú ÏÉùÏÑ± ÏôÑÎ£å! (Î™®Îç∏: {result.get('model_used', 'Unknown')})")
                                st.balloons()
                            else:
                                st.error(f"‚ùå Ïò§Î•ò: {result.get('error')}")
                    else:
                        st.warning("ÏöîÍµ¨ÏÇ¨Ìï≠ÏùÑ ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.")
            
            with col2:
                st.markdown("### üìÑ ÏÉùÏÑ±Îêú Î¨∏ÏÑú")
                
                if st.session_state.current_result and st.session_state.current_result.get("success"):
                    result = st.session_state.current_result
                    
                    # Show model info
                    st.markdown(f"**ÏÇ¨Ïö©Îêú Î™®Îç∏**: {result.get('provider', 'Unknown')} - {result.get('model_used', 'Unknown')}")
                    
                    st.text_area(
                        "ÏµúÏ¢Ö Î¨∏ÏÑú",
                        value=result.get("final_document", ""),
                        height=350,
                        label_visibility="collapsed"
                    )
                    
                    col2_1, col2_2, col2_3 = st.columns(3)
                    with col2_1:
                        st.metric("ÌíàÏßà Ï†êÏàò", f"{result.get('quality_score', 0):.1%}")
                    with col2_2:
                        st.metric("Ï≤òÎ¶¨ ÏãúÍ∞Ñ", f"{result.get('total_time', 0):.1f}Ï¥à")
                    with col2_3:
                        st.metric("Î™®Îç∏", result.get('provider', '-'))
                    
                    st.download_button(
                        label="üì• Î¨∏ÏÑú Îã§Ïö¥Î°úÎìú",
                        data=result["final_document"],
                        file_name=f"document_{result.get('provider', 'ai')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
                        mime="text/plain"
                    )
                else:
                    st.info("Î¨∏ÏÑúÎ•º ÏÉùÏÑ±ÌïòÎ†§Î©¥ ÏôºÏ™ΩÏóêÏÑú ÏöîÍµ¨ÏÇ¨Ìï≠ÏùÑ ÏûÖÎ†•ÌïòÍ≥† 'Î¨∏ÏÑú ÏÉùÏÑ±' Î≤ÑÌäºÏùÑ ÌÅ¥Î¶≠ÌïòÏÑ∏Ïöî.")
        
        with tab2:
            st.markdown("### üîç Ïó¨Îü¨ Î™®Îç∏ ÎπÑÍµê")
            st.info("ÎèôÏùºÌïú ÏöîÍµ¨ÏÇ¨Ìï≠ÏúºÎ°ú Ïó¨Îü¨ AI Î™®Îç∏Ïùò Í≤∞Í≥ºÎ•º ÎπÑÍµêÌï¥Î≥¥ÏÑ∏Ïöî.")
            
            compare_requirements = st.text_area(
                "ÎπÑÍµêÌï† Î¨∏ÏÑú ÏöîÍµ¨ÏÇ¨Ìï≠",
                placeholder="Î™®Îì† Î™®Îç∏ÏóêÏÑú ÌÖåÏä§Ìä∏Ìï† ÏöîÍµ¨ÏÇ¨Ìï≠ÏùÑ ÏûÖÎ†•ÌïòÏÑ∏Ïöî...",
                height=150
            )
            
            if st.button("üî¨ Î™®Îì† Î™®Îç∏Î°ú ÏÉùÏÑ±", type="primary"):
                if compare_requirements:
                    if not self.multi_model_agent:
                        agent_config = config.get_agent_config()
                        self.multi_model_agent = MultiModelAgent(agent_config)
                    
                    # Create input data
                    input_data = {
                        "document_type": doc_type,
                        "requirements": compare_requirements,
                        "tone": tone,
                        "temperature": temperature,
                        "max_tokens": max_tokens
                    }
                    
                    prompt = self._create_prompt(input_data)
                    
                    # Compare across all available providers
                    with st.spinner("Î™®Îì† Î™®Îç∏Î°ú Î¨∏ÏÑúÎ•º ÏÉùÏÑ± Ï§ëÏûÖÎãàÎã§..."):
                        results = self.multi_model_agent.compare_models(prompt)
                    
                    # Display results
                    for provider, response in results.items():
                        with st.expander(f"{provider} - {response.model_used}"):
                            st.text_area(
                                f"{provider} Í≤∞Í≥º",
                                value=response.content,
                                height=300,
                                label_visibility="collapsed"
                            )
                            
                            # Calculate quality score
                            term_validation = validate_financial_terms(response.content)
                            compliance_check = check_compliance(response.content, doc_type)
                            quality_score = calculate_quality_score(response.content, term_validation, compliance_check)
                            
                            st.metric("ÌíàÏßà Ï†êÏàò", f"{quality_score:.1%}")
                else:
                    st.warning("ÎπÑÍµêÌï† ÏöîÍµ¨ÏÇ¨Ìï≠ÏùÑ ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.")
        
        with tab3:
            if st.session_state.current_result and st.session_state.current_result.get("success"):
                result = st.session_state.current_result
                
                st.markdown("### üìä Î¨∏ÏÑú Î∂ÑÏÑù")
                
                # Quality metrics
                st.markdown("#### ÌíàÏßà ÏßÄÌëú")
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.metric("Ï†ÑÏ≤¥ ÌíàÏßà", f"{result.get('quality_score', 0):.1%}")
                
                with col2:
                    validation = result.get('validation', {})
                    st.metric("Ïö©Ïñ¥ Ï†ïÌôïÎèÑ", f"{validation.get('terms', {}).get('score', 0):.1%}")
                
                with col3:
                    st.metric("Í∑úÏ†ï Ï§ÄÏàò", "‚úÖ Ï§ÄÏàò" if validation.get('compliance', {}).get('is_compliant') else "‚ùå ÎØ∏Ï§ÄÏàò")
                
                with col4:
                    st.metric("ÏÇ¨Ïö© Î™®Îç∏", result.get('provider', '-'))
                
                # Document stats
                st.markdown("#### Î¨∏ÏÑú ÌÜµÍ≥Ñ")
                doc = result.get('final_document', '')
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("Î¨∏Ïûê Ïàò", f"{len(doc):,}")
                with col2:
                    st.metric("Îã®Ïñ¥ Ïàò", f"{len(doc.split()):,}")
                with col3:
                    st.metric("Î¨∏Ïû• Ïàò", f"{doc.count('.') + doc.count('!') + doc.count('?'):,}")
            else:
                st.info("Î®ºÏ†Ä Î¨∏ÏÑúÎ•º ÏÉùÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî.")
        
        with tab4:
            if st.session_state.history:
                st.markdown("### üìö Î¨∏ÏÑú ÏÉùÏÑ± Ïù¥Î†•")
                
                for idx, item in enumerate(reversed(st.session_state.history), 1):
                    timestamp = datetime.fromisoformat(item['timestamp'])
                    result = item['result']
                    
                    with st.expander(
                        f"Î¨∏ÏÑú #{len(st.session_state.history) - idx + 1} - "
                        f"{timestamp.strftime('%Y-%m-%d %H:%M')} - "
                        f"{result.get('provider', 'Unknown')}"
                    ):
                        st.write(f"**Î¨∏ÏÑú Ïú†Ìòï**: {item['input']['document_type']}")
                        st.write(f"**ÌÜ§**: {item['input']['tone']}")
                        st.write(f"**Î™®Îç∏**: {result.get('provider', '-')} - {result.get('model_used', '-')}")
                        st.write(f"**ÌíàÏßà Ï†êÏàò**: {result.get('quality_score', 0):.1%}")
                        st.write(f"**Ï≤òÎ¶¨ ÏãúÍ∞Ñ**: {result.get('total_time', 0):.1f}Ï¥à")
                        
                        if result.get('success'):
                            st.text_area(
                                "Î¨∏ÏÑú ÎÇ¥Ïö©",
                                value=result.get('final_document', ''),
                                height=200,
                                label_visibility="collapsed"
                            )
            else:
                st.info("ÏïÑÏßÅ ÏÉùÏÑ±Îêú Î¨∏ÏÑúÍ∞Ä ÏóÜÏäµÎãàÎã§.")


def main():
    """Main entry point"""
    app = MultiModelFinancialWritingApp()
    app.run()


if __name__ == "__main__":
    main()