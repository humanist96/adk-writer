"""
Multi-Model AI Financial Writer Pro - Streamlit Cloud Version
Supports Anthropic Claude, OpenAI GPT, and Google Gemini
"""

import streamlit as st
from typing import Dict, Any
import json
from datetime import datetime
from pathlib import Path
import time

# Try cloud config first, fallback to local
try:
    from src.config_cloud import config
except:
    from src.config import config

from src.agents.multi_model_agents import (
    ModelFactory,
    MultiModelAgent,
    MultiModelAgentResponse
)
from src.agents.loop_agent import LoopAgent
from src.tools.custom_tools import (
    validate_financial_terms,
    check_compliance,
    calculate_quality_score
)

# Configure page
st.set_page_config(
    page_title="AI Financial Writer Pro - Multi Model",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .stApp {
        font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
    }
    
    .main-header {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 2rem;
        border-radius: 16px;
        margin-bottom: 2rem;
        color: white;
    }
    
    .model-card {
        background: white;
        border-radius: 12px;
        padding: 1rem;
        border: 2px solid #e2e8f0;
        margin-bottom: 1rem;
        transition: all 0.3s ease;
    }
    
    .model-card:hover {
        border-color: #667eea;
        box-shadow: 0 4px 12px rgba(102, 126, 234, 0.15);
    }
    
    .provider-badge {
        display: inline-block;
        padding: 0.25rem 0.75rem;
        border-radius: 20px;
        font-size: 0.875rem;
        font-weight: 600;
        margin-right: 0.5rem;
    }
    
    .anthropic-badge {
        background: linear-gradient(135deg, #FF6B6B 0%, #4ECDC4 100%);
        color: white;
    }
    
    .openai-badge {
        background: linear-gradient(135deg, #74b9ff 0%, #0984e3 100%);
        color: white;
    }
    
    .google-badge {
        background: linear-gradient(135deg, #4285f4 0%, #34a853 100%);
        color: white;
    }
    
    .stat-card {
        background: white;
        border-radius: 12px;
        padding: 1.5rem;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        margin-bottom: 1rem;
    }
    
    .metric-value {
        font-size: 2rem;
        font-weight: 700;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
    }
    
    .comparison-panel {
        background: white;
        border-radius: 12px;
        padding: 1.5rem;
        border: 2px solid #e2e8f0;
        margin-bottom: 1rem;
    }
    
    .stButton > button {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        border: none;
        padding: 0.75rem 2rem;
        border-radius: 12px;
        font-weight: 600;
        width: 100%;
    }
    
    .stButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 0 10px 20px rgba(102, 126, 234, 0.3);
    }
</style>
""", unsafe_allow_html=True)

class MultiModelFinancialWritingApp:
    """Multi-Model Financial Writing AI Application"""
    
    def __init__(self):
        self.config = config
        self.multi_model_agent = None
        self._initialize_session_state()
    
    def _initialize_session_state(self):
        """Initialize session state variables"""
        if 'history' not in st.session_state:
            st.session_state.history = []
        if 'current_result' not in st.session_state:
            st.session_state.current_result = None
        if 'stats' not in st.session_state:
            st.session_state.stats = {
                'total_documents': 0,
                'avg_quality': 0,
                'avg_iterations': 0,
                'total_time': 0,
                'models_used': {}
            }
        if 'selected_provider' not in st.session_state:
            st.session_state.selected_provider = config.DEFAULT_PROVIDER
        if 'selected_model' not in st.session_state:
            st.session_state.selected_model = None
    
    def render_header(self):
        """Render header"""
        st.markdown("""
        <div class="main-header">
            <h1 style="margin: 0; font-size: 2.5rem;">ğŸ¤– AI Financial Writer Pro - Multi Model</h1>
            <p style="margin-top: 0.5rem; opacity: 0.9;">
                ì°¨ì„¸ëŒ€ ê¸ˆìœµ ë¬¸ì„œ ì‘ì„± AI | Anthropic Claude â€¢ OpenAI GPT â€¢ Google Gemini
            </p>
        </div>
        """, unsafe_allow_html=True)
    
    def render_model_selector(self):
        """Render model selection UI"""
        st.markdown("## ğŸ¯ AI ëª¨ë¸ ì„ íƒ")
        
        # Get available models
        available_models = ModelFactory.get_available_models()
        
        # Provider selection
        col1, col2, col3 = st.columns(3)
        
        providers_available = []
        
        with col1:
            if config.ANTHROPIC_API_KEY:
                providers_available.append("Anthropic")
                if st.button("ğŸ”´ Anthropic Claude", type="secondary" if st.session_state.selected_provider != "Anthropic" else "primary"):
                    st.session_state.selected_provider = "Anthropic"
                    st.rerun()
        
        with col2:
            if config.OPENAI_API_KEY:
                providers_available.append("OpenAI")
                if st.button("ğŸ”µ OpenAI GPT", type="secondary" if st.session_state.selected_provider != "OpenAI" else "primary"):
                    st.session_state.selected_provider = "OpenAI"
                    st.rerun()
        
        with col3:
            if config.GOOGLE_API_KEY:
                providers_available.append("Google")
                if st.button("ğŸŸ¢ Google Gemini", type="secondary" if st.session_state.selected_provider != "Google" else "primary"):
                    st.session_state.selected_provider = "Google"
                    st.rerun()
        
        # Model selection for current provider
        if st.session_state.selected_provider and st.session_state.selected_provider in available_models:
            st.markdown(f"### ëª¨ë¸ ì„ íƒ: {st.session_state.selected_provider}")
            
            models = available_models[st.session_state.selected_provider]
            
            # Create model cards
            cols = st.columns(2)
            for idx, (model_id, model_name) in enumerate(models.items()):
                with cols[idx % 2]:
                    if st.button(
                        f"{model_name}",
                        key=f"model_{model_id}",
                        help=f"ëª¨ë¸ ID: {model_id}"
                    ):
                        st.session_state.selected_model = model_id
                        st.success(f"âœ… {model_name} ì„ íƒë¨")
            
            # Show current selection
            if st.session_state.selected_model:
                st.info(f"í˜„ì¬ ì„ íƒëœ ëª¨ë¸: **{models.get(st.session_state.selected_model, st.session_state.selected_model)}**")
        
        return providers_available
    
    def render_stats(self):
        """Render statistics"""
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric(
                "ì´ ìƒì„± ë¬¸ì„œ",
                st.session_state.stats['total_documents'],
                "+1" if st.session_state.stats['total_documents'] > 0 else None
            )
        
        with col2:
            st.metric(
                "í‰ê·  í’ˆì§ˆ",
                f"{st.session_state.stats['avg_quality']:.1%}",
                f"+{(st.session_state.stats['avg_quality'] - 0.7) * 100:.0f}%" if st.session_state.stats['avg_quality'] > 0 else None
            )
        
        with col3:
            st.metric(
                "í‰ê·  ë°˜ë³µ",
                f"{st.session_state.stats['avg_iterations']:.1f}íšŒ",
                None
            )
        
        with col4:
            # Show most used model
            if st.session_state.stats['models_used']:
                most_used = max(st.session_state.stats['models_used'].items(), key=lambda x: x[1])
                st.metric(
                    "ì£¼ë¡œ ì‚¬ìš©ëœ ëª¨ë¸",
                    most_used[0],
                    f"{most_used[1]}íšŒ ì‚¬ìš©"
                )
            else:
                st.metric("ì£¼ë¡œ ì‚¬ìš©ëœ ëª¨ë¸", "-", None)
    
    def process_document(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Process document through the pipeline"""
        try:
            # Initialize multi-model agent if not already done
            if not self.multi_model_agent:
                agent_config = config.get_agent_config()
                self.multi_model_agent = MultiModelAgent(agent_config)
            
            # Show progress
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # Get selected provider and model
            provider = st.session_state.selected_provider
            model = st.session_state.selected_model
            
            if model:
                # Update config with selected model
                if provider == "Anthropic":
                    self.multi_model_agent.config['anthropic_model'] = model
                elif provider == "OpenAI":
                    self.multi_model_agent.config['openai_model'] = model
                elif provider == "Google":
                    self.multi_model_agent.config['google_model'] = model
            
            # Generate document using multi-model agent
            status_text.text(f"ëª¨ë¸ ì¤€ë¹„ ì¤‘... ({provider})")
            progress_bar.progress(20)
            
            # Create prompt for document generation
            prompt = self._create_prompt(input_data)
            
            status_text.text(f"ë¬¸ì„œ ìƒì„± ì¤‘... ({provider})")
            progress_bar.progress(50)
            
            # Generate response
            response = self.multi_model_agent.generate(
                prompt,
                provider=provider,
                temperature=input_data.get('temperature', 0.7),
                max_tokens=input_data.get('max_tokens', 2048)
            )
            
            progress_bar.progress(80)
            status_text.text("í’ˆì§ˆ ê²€ì¦ ì¤‘...")
            
            # Validation
            final_doc = response.content
            term_validation = validate_financial_terms(final_doc)
            compliance_check = check_compliance(final_doc, input_data.get("document_type", "email"))
            final_score = calculate_quality_score(final_doc, term_validation, compliance_check)
            
            progress_bar.progress(100)
            status_text.text("ì™„ë£Œ!")
            
            # Update model usage stats
            model_key = f"{provider} - {response.model_used}"
            if model_key not in st.session_state.stats['models_used']:
                st.session_state.stats['models_used'][model_key] = 0
            st.session_state.stats['models_used'][model_key] += 1
            
            result = {
                "success": True,
                "final_document": final_doc,
                "quality_score": final_score,
                "iterations": 1,
                "total_time": 5.0,
                "provider": provider,
                "model_used": response.model_used,
                "validation": {
                    "terms": term_validation,
                    "compliance": compliance_check,
                    "final_score": final_score
                }
            }
            
            # Update stats
            self._update_stats(result)
            
            return result
            
        except Exception as e:
            return {"error": str(e), "success": False}
    
    def _create_prompt(self, input_data: Dict[str, Any]) -> str:
        """Create prompt for document generation"""
        doc_type = config.DOCUMENT_TYPES.get(input_data['document_type'], input_data['document_type'])
        
        prompt = f"""ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ê¸ˆìœµ ë¬¸ì„œ ì‘ì„± AIì…ë‹ˆë‹¤.
        
ë‹¤ìŒ ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” {doc_type}ì„(ë¥¼) ì‘ì„±í•´ì£¼ì„¸ìš”:

ìš”êµ¬ì‚¬í•­: {input_data['requirements']}
í†¤ì•¤ë§¤ë„ˆ: {input_data['tone']}
"""
        
        if input_data.get('recipient'):
            prompt += f"\nìˆ˜ì‹ ì: {input_data['recipient']}"
        
        if input_data.get('subject'):
            prompt += f"\nì œëª©: {input_data['subject']}"
        
        if input_data.get('additional_context'):
            prompt += f"\nì¶”ê°€ ì»¨í…ìŠ¤íŠ¸: {input_data['additional_context']}"
        
        prompt += """

ë‹¤ìŒ ê¸°ì¤€ì„ ì¶©ì¡±í•˜ëŠ” ì „ë¬¸ì ì´ê³  ì •í™•í•œ ë¬¸ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:
1. ê¸ˆìœµ ì „ë¬¸ ìš©ì–´ë¥¼ ì •í™•í•˜ê²Œ ì‚¬ìš©
2. ê·œì • ì¤€ìˆ˜ ë° ë²•ì  ìš”êµ¬ì‚¬í•­ ì¶©ì¡±
3. ëª…í™•í•˜ê³  ê°„ê²°í•œ ë¬¸ì¥ êµ¬ì„±
4. ì ì ˆí•œ êµ¬ì¡°ì™€ í˜•ì‹
5. ì „ë¬¸ì ì´ë©´ì„œë„ ì´í•´í•˜ê¸° ì‰¬ìš´ í‘œí˜„
"""
        
        return prompt
    
    def _update_stats(self, result: Dict[str, Any]):
        """Update statistics"""
        stats = st.session_state.stats
        stats['total_documents'] += 1
        
        current_quality = result.get('quality_score', 0)
        stats['avg_quality'] = (
            (stats['avg_quality'] * (stats['total_documents'] - 1) + current_quality) 
            / stats['total_documents']
        )
        
        current_iter = result.get('iterations', 0)
        stats['avg_iterations'] = (
            (stats['avg_iterations'] * (stats['total_documents'] - 1) + current_iter)
            / stats['total_documents']
        )
        
        stats['total_time'] += result.get('total_time', 0)
    
    def run(self):
        """Run the application"""
        # Header
        self.render_header()
        
        # Model selector
        providers_available = self.render_model_selector()
        
        if not providers_available:
            st.error("âš ï¸ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Streamlit secrets ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
            st.stop()
        
        st.divider()
        
        # Stats
        self.render_stats()
        
        # Sidebar
        with st.sidebar:
            st.markdown("## âš™ï¸ ì„¤ì •")
            
            # Model settings
            st.markdown("### ğŸ¤– ëª¨ë¸ ì„¤ì •")
            
            temperature = st.slider(
                "Temperature (ì°½ì˜ì„±)",
                min_value=0.0,
                max_value=1.0,
                value=config.TEMPERATURE,
                step=0.1,
                help="ë‚®ì„ìˆ˜ë¡ ì¼ê´€ì„± ìˆê³ , ë†’ì„ìˆ˜ë¡ ì°½ì˜ì ì…ë‹ˆë‹¤"
            )
            
            max_tokens = st.number_input(
                "ìµœëŒ€ í† í° ìˆ˜",
                min_value=500,
                max_value=4000,
                value=config.MAX_OUTPUT_TOKENS,
                step=100
            )
            
            st.divider()
            
            # Document settings
            st.markdown("### ğŸ“„ ë¬¸ì„œ ì„¤ì •")
            
            doc_type = st.selectbox(
                "ë¬¸ì„œ ìœ í˜•",
                options=list(config.DOCUMENT_TYPES.keys()),
                format_func=lambda x: config.DOCUMENT_TYPES[x]
            )
            
            tone = st.select_slider(
                "í†¤ì•¤ë§¤ë„ˆ",
                options=["formal", "professional", "professional_friendly", "friendly"],
                value="professional",
                format_func=lambda x: {
                    "formal": "ê²©ì‹ìˆëŠ”",
                    "professional": "ì „ë¬¸ì ì¸",
                    "professional_friendly": "ì „ë¬¸ì ì´ë©´ì„œ ì¹œê·¼í•œ",
                    "friendly": "ì¹œê·¼í•œ"
                }.get(x, x)
            )
            
            st.divider()
            
            with st.expander("ğŸ¨ ëª¨ë¸ë³„ íŠ¹ì§•"):
                st.markdown("""
                **Anthropic Claude**
                - ê¸´ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ ìš°ìˆ˜
                - ë…¼ë¦¬ì ì´ê³  ì²´ê³„ì ì¸ ì‘ì„±
                - ë³µì¡í•œ ë¶„ì„ì— ê°•ì 
                
                **OpenAI GPT**
                - ë²”ìš©ì„±ê³¼ ê· í˜•ì¡íŒ ì„±ëŠ¥
                - ì°½ì˜ì ì¸ ì‘ì„±ì— ê°•ì 
                - ë‹¤ì–‘í•œ ìŠ¤íƒ€ì¼ ì§€ì›
                
                **Google Gemini**
                - ë¹ ë¥¸ ì‘ë‹µ ì†ë„
                - íš¨ìœ¨ì ì¸ ì²˜ë¦¬
                - ë©€í‹°ëª¨ë‹¬ ì§€ì›
                """)
            
            st.divider()
            
            if st.button("ğŸ”„ ì´ˆê¸°í™”"):
                st.session_state.history = []
                st.session_state.current_result = None
                st.rerun()
        
        # Main content
        tab1, tab2, tab3, tab4 = st.tabs(["âœï¸ ë¬¸ì„œ ì‘ì„±", "ğŸ” ëª¨ë¸ ë¹„êµ", "ğŸ“Š ë¹„êµ ë¶„ì„", "ğŸ“š ì´ë ¥"])
        
        with tab1:
            col1, col2 = st.columns([1, 1])
            
            with col1:
                st.markdown("### ğŸ“ ë¬¸ì„œ ìš”êµ¬ì‚¬í•­")
                
                requirements = st.text_area(
                    "ìš”êµ¬ì‚¬í•­",
                    placeholder="ì‘ì„±í•˜ê³ ì í•˜ëŠ” ë¬¸ì„œì˜ ë‚´ìš©ê³¼ ìš”êµ¬ì‚¬í•­ì„ ì…ë ¥í•˜ì„¸ìš”...",
                    height=200,
                    label_visibility="collapsed"
                )
                
                with st.expander("ì¶”ê°€ ì •ë³´"):
                    recipient = st.text_input("ìˆ˜ì‹ ì")
                    subject = st.text_input("ì œëª©")
                    additional_context = st.text_area("ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸", height=100)
                
                if st.button("ğŸš€ ë¬¸ì„œ ìƒì„±", type="primary"):
                    if requirements:
                        if not st.session_state.selected_model:
                            st.warning("ë¨¼ì € AI ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
                        else:
                            input_data = {
                                "document_type": doc_type,
                                "requirements": requirements,
                                "tone": tone,
                                "recipient": recipient,
                                "subject": subject,
                                "additional_context": additional_context,
                                "temperature": temperature,
                                "max_tokens": max_tokens
                            }
                            
                            with st.spinner(f"{st.session_state.selected_provider} AIê°€ ë¬¸ì„œë¥¼ ì‘ì„± ì¤‘ì…ë‹ˆë‹¤..."):
                                result = self.process_document(input_data)
                            
                            st.session_state.current_result = result
                            st.session_state.history.append({
                                "timestamp": datetime.now().isoformat(),
                                "input": input_data,
                                "result": result
                            })
                            
                            if result.get("success"):
                                st.success(f"âœ… ë¬¸ì„œ ìƒì„± ì™„ë£Œ! (ëª¨ë¸: {result.get('model_used', 'Unknown')})")
                                st.balloons()
                            else:
                                st.error(f"âŒ ì˜¤ë¥˜: {result.get('error')}")
                    else:
                        st.warning("ìš”êµ¬ì‚¬í•­ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            
            with col2:
                st.markdown("### ğŸ“„ ìƒì„±ëœ ë¬¸ì„œ")
                
                if st.session_state.current_result and st.session_state.current_result.get("success"):
                    result = st.session_state.current_result
                    
                    # Show model info
                    st.markdown(f"**ì‚¬ìš©ëœ ëª¨ë¸**: {result.get('provider', 'Unknown')} - {result.get('model_used', 'Unknown')}")
                    
                    st.text_area(
                        "ìµœì¢… ë¬¸ì„œ",
                        value=result.get("final_document", ""),
                        height=350,
                        label_visibility="collapsed"
                    )
                    
                    col2_1, col2_2, col2_3 = st.columns(3)
                    with col2_1:
                        st.metric("í’ˆì§ˆ ì ìˆ˜", f"{result.get('quality_score', 0):.1%}")
                    with col2_2:
                        st.metric("ì²˜ë¦¬ ì‹œê°„", f"{result.get('total_time', 0):.1f}ì´ˆ")
                    with col2_3:
                        st.metric("ëª¨ë¸", result.get('provider', '-'))
                    
                    st.download_button(
                        label="ğŸ“¥ ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ",
                        data=result["final_document"],
                        file_name=f"document_{result.get('provider', 'ai')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
                        mime="text/plain"
                    )
                else:
                    st.info("ë¬¸ì„œë¥¼ ìƒì„±í•˜ë ¤ë©´ ì™¼ìª½ì—ì„œ ìš”êµ¬ì‚¬í•­ì„ ì…ë ¥í•˜ê³  'ë¬¸ì„œ ìƒì„±' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")
        
        with tab2:
            st.markdown("### ğŸ” ì—¬ëŸ¬ ëª¨ë¸ ë¹„êµ")
            st.info("ë™ì¼í•œ ìš”êµ¬ì‚¬í•­ìœ¼ë¡œ ì—¬ëŸ¬ AI ëª¨ë¸ì˜ ê²°ê³¼ë¥¼ ë¹„êµí•´ë³´ì„¸ìš”.")
            
            compare_requirements = st.text_area(
                "ë¹„êµí•  ë¬¸ì„œ ìš”êµ¬ì‚¬í•­",
                placeholder="ëª¨ë“  ëª¨ë¸ì—ì„œ í…ŒìŠ¤íŠ¸í•  ìš”êµ¬ì‚¬í•­ì„ ì…ë ¥í•˜ì„¸ìš”...",
                height=150
            )
            
            if st.button("ğŸ”¬ ëª¨ë“  ëª¨ë¸ë¡œ ìƒì„±", type="primary"):
                if compare_requirements:
                    if not self.multi_model_agent:
                        agent_config = config.get_agent_config()
                        self.multi_model_agent = MultiModelAgent(agent_config)
                    
                    # Create input data
                    input_data = {
                        "document_type": doc_type,
                        "requirements": compare_requirements,
                        "tone": tone,
                        "temperature": temperature,
                        "max_tokens": max_tokens
                    }
                    
                    prompt = self._create_prompt(input_data)
                    
                    # Compare across all available providers
                    with st.spinner("ëª¨ë“  ëª¨ë¸ë¡œ ë¬¸ì„œë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
                        results = self.multi_model_agent.compare_models(prompt)
                    
                    # Display results
                    for provider, response in results.items():
                        with st.expander(f"{provider} - {response.model_used}"):
                            st.text_area(
                                f"{provider} ê²°ê³¼",
                                value=response.content,
                                height=300,
                                label_visibility="collapsed"
                            )
                            
                            # Calculate quality score
                            term_validation = validate_financial_terms(response.content)
                            compliance_check = check_compliance(response.content, doc_type)
                            quality_score = calculate_quality_score(response.content, term_validation, compliance_check)
                            
                            st.metric("í’ˆì§ˆ ì ìˆ˜", f"{quality_score:.1%}")
                else:
                    st.warning("ë¹„êµí•  ìš”êµ¬ì‚¬í•­ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        with tab3:
            if st.session_state.current_result and st.session_state.current_result.get("success"):
                result = st.session_state.current_result
                
                st.markdown("### ğŸ“Š ë¬¸ì„œ ë¶„ì„")
                
                # Quality metrics
                st.markdown("#### í’ˆì§ˆ ì§€í‘œ")
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.metric("ì „ì²´ í’ˆì§ˆ", f"{result.get('quality_score', 0):.1%}")
                
                with col2:
                    validation = result.get('validation', {})
                    st.metric("ìš©ì–´ ì •í™•ë„", f"{validation.get('terms', {}).get('score', 0):.1%}")
                
                with col3:
                    st.metric("ê·œì • ì¤€ìˆ˜", "âœ… ì¤€ìˆ˜" if validation.get('compliance', {}).get('is_compliant') else "âŒ ë¯¸ì¤€ìˆ˜")
                
                with col4:
                    st.metric("ì‚¬ìš© ëª¨ë¸", result.get('provider', '-'))
                
                # Document stats
                st.markdown("#### ë¬¸ì„œ í†µê³„")
                doc = result.get('final_document', '')
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("ë¬¸ì ìˆ˜", f"{len(doc):,}")
                with col2:
                    st.metric("ë‹¨ì–´ ìˆ˜", f"{len(doc.split()):,}")
                with col3:
                    st.metric("ë¬¸ì¥ ìˆ˜", f"{doc.count('.') + doc.count('!') + doc.count('?'):,}")
            else:
                st.info("ë¨¼ì € ë¬¸ì„œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.")
        
        with tab4:
            if st.session_state.history:
                st.markdown("### ğŸ“š ë¬¸ì„œ ìƒì„± ì´ë ¥")
                
                for idx, item in enumerate(reversed(st.session_state.history), 1):
                    timestamp = datetime.fromisoformat(item['timestamp'])
                    result = item['result']
                    
                    with st.expander(
                        f"ë¬¸ì„œ #{len(st.session_state.history) - idx + 1} - "
                        f"{timestamp.strftime('%Y-%m-%d %H:%M')} - "
                        f"{result.get('provider', 'Unknown')}"
                    ):
                        st.write(f"**ë¬¸ì„œ ìœ í˜•**: {item['input']['document_type']}")
                        st.write(f"**í†¤**: {item['input']['tone']}")
                        st.write(f"**ëª¨ë¸**: {result.get('provider', '-')} - {result.get('model_used', '-')}")
                        st.write(f"**í’ˆì§ˆ ì ìˆ˜**: {result.get('quality_score', 0):.1%}")
                        st.write(f"**ì²˜ë¦¬ ì‹œê°„**: {result.get('total_time', 0):.1f}ì´ˆ")
                        
                        if result.get('success'):
                            st.text_area(
                                "ë¬¸ì„œ ë‚´ìš©",
                                value=result.get('final_document', ''),
                                height=200,
                                label_visibility="collapsed"
                            )
            else:
                st.info("ì•„ì§ ìƒì„±ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")


def main():
    """Main entry point"""
    app = MultiModelFinancialWritingApp()
    app.run()


if __name__ == "__main__":
    main()