"""
Base agents for the writing pipeline using Google ADK
"""

from typing import Dict, Any, Optional, List
from dataclasses import dataclass, field
import json
import google.generativeai as genai
from loguru import logger
try:
    from .multi_model_agents import MultiModelAgent
except ImportError:
    MultiModelAgent = None

@dataclass
class AgentResponse:
    """Standardized response format for all agents"""
    content: str
    metadata: Dict[str, Any]
    quality_score: float = 0.0
    issues_found: List[str] = field(default_factory=list)
    suggestions: List[str] = field(default_factory=list)
    web_search_results: Optional[Dict[str, Any]] = None
    
    def to_dict(self) -> Dict[str, Any]:
        response_dict = {
            "content": self.content,
            "metadata": self.metadata,
            "quality_score": self.quality_score,
            "issues_found": self.issues_found or [],
            "suggestions": self.suggestions or []
        }
        
        # Include web search results if available
        if self.web_search_results:
            response_dict["web_search_results"] = self.web_search_results
            
        return response_dict


class BaseLlmAgent:
    """Base class for all LLM agents"""
    
    def __init__(self, name: str, model_config: Dict[str, Any]):
        self.name = name
        self.model_config = model_config
        self._setup_model()
        
    def _setup_model(self):
        """Initialize the AI model (multi-model or Google Gemini)"""
        # Check if we should use multi-model support
        provider = self.model_config.get("provider")
        if provider and MultiModelAgent:
            # Use multi-model agent for Anthropic/OpenAI
            self.multi_model_agent = MultiModelAgent(self.model_config)
            self.model = None
            # Log the actual model being used
            if provider == "Anthropic":
                model_used = self.model_config.get("anthropic_model", "claude-3-5-sonnet-20241022")
            elif provider == "OpenAI":
                model_used = self.model_config.get("openai_model", "gpt-4-turbo")
            else:
                model_used = self.model_config.get("google_model", "gemini-1.5-flash")
            logger.info(f"Initialized {self.name} with multi-model support ({provider}: {model_used})")
        else:
            # Use Google Gemini by default
            genai.configure(api_key=self.model_config.get("api_key"))
            model_name = self.model_config.get("model", "gemini-1.5-flash")
            # Ensure we use Gemini models only for Google API
            if model_name.startswith("claude") or model_name.startswith("gpt"):
                model_name = "gemini-1.5-flash"
            self.model = genai.GenerativeModel(
                model_name=model_name,
                generation_config={
                    "temperature": self.model_config.get("temperature", 0.7),
                    "max_output_tokens": self.model_config.get("max_output_tokens", 2048),
                }
            )
            self.multi_model_agent = None
            logger.info(f"Initialized {self.name} with model {model_name}")
    
    def generate(self, prompt: str, context: Optional[Dict[str, Any]] = None) -> str:
        """Generate response using the LLM"""
        try:
            if self.multi_model_agent:
                # Use multi-model agent
                provider = self.model_config.get("provider", "Google")
                response = self.multi_model_agent.generate(prompt, provider=provider)
                return response.content
            else:
                # Use Google Gemini
                response = self.model.generate_content(prompt)
                return response.text
        except Exception as e:
            logger.error(f"Error in {self.name}: {str(e)}")
            raise
    
    def process(self, input_data: Dict[str, Any]) -> AgentResponse:
        """Process input and return structured response"""
        raise NotImplementedError("Subclasses must implement process method")


class DraftWriterAgent(BaseLlmAgent):
    """Agent responsible for creating initial drafts"""
    
    def __init__(self, model_config: Dict[str, Any]):
        super().__init__("DraftWriterAgent", model_config)
        self.templates = self._load_templates()
    
    def _load_templates(self) -> Dict[str, str]:
        """Load document templates"""
        return {
            "email": """
ì£¼ì œ: {subject}
ìˆ˜ì‹ : {recipient}
ë°œì‹ : {sender}

ì•ˆë…•í•˜ì‹­ë‹ˆê¹Œ, {recipient}ë‹˜

{content}

ê°ì‚¬í•©ë‹ˆë‹¤.
{sender} ë“œë¦¼
            """,
            "proposal": """
[íˆ¬ìž ì œì•ˆì„œ]

ì œëª©: {title}
ìž‘ì„±ì¼: {date}
ëŒ€ìƒ: {target}

1. ì œì•ˆ ê°œìš”
{overview}

2. íˆ¬ìž ìƒí’ˆ ì„¤ëª…
{product_description}

3. ì˜ˆìƒ ìˆ˜ìµë¥ 
{expected_returns}

4. ë¦¬ìŠ¤í¬ ê´€ë¦¬
{risk_management}

5. ê²°ë¡ 
{conclusion}
            """,
            "official_letter": """
[ê³µì‹ ë¬¸ì„œ]

ë¬¸ì„œë²ˆí˜¸: {doc_number}
ë‚ ì§œ: {date}
ìˆ˜ì‹ : {recipient}
ë°œì‹ : {sender}
ì œëª©: {subject}

{content}

[ì„œëª…]
{signature}
            """
        }
    
    def process(self, input_data: Dict[str, Any]) -> AgentResponse:
        """Generate initial draft based on input"""
        from datetime import datetime
        current_date = datetime.now().strftime("%Yë…„ %mì›” %dì¼")
        current_year = datetime.now().year
        
        doc_type = input_data.get("document_type", "email")
        requirements = input_data.get("requirements", "")
        tone = input_data.get("tone", "professional")
        recipient = input_data.get("recipient", "")
        subject = input_data.get("subject", "")
        additional_context = input_data.get("additional_context", "")
        
        # Build comprehensive prompt with all context
        prompt = f"""
ë‹¹ì‹ ì€ ì½”ìŠ¤ì½¤ ê¸ˆìœµì˜ì—…ë¶€ì˜ ì „ë¬¸ ë¬¸ì„œ ìž‘ì„±ìžìž…ë‹ˆë‹¤.
í˜„ìž¬ ì‹œì ì€ {current_date} ({current_year}ë…„)ìž…ë‹ˆë‹¤.
ë‹¤ìŒ ìš”êµ¬ì‚¬í•­ê³¼ ì¶”ê°€ ì •ë³´ë¥¼ ëª¨ë‘ ë°˜ì˜í•˜ì—¬ {doc_type} ë¬¸ì„œì˜ ì´ˆì•ˆì„ ìž‘ì„±í•´ì£¼ì„¸ìš”.

âš ï¸ ì¤‘ìš”: ë°˜ë“œì‹œ {current_year}ë…„ í˜„ìž¬ ì‹œì  ê¸°ì¤€ìœ¼ë¡œ ìž‘ì„±í•˜ì„¸ìš”.
- "ì˜¬í•´"ëŠ” {current_year}ë…„ì„ ì˜ë¯¸í•©ë‹ˆë‹¤
- "ìž‘ë…„"ì€ {current_year-1}ë…„ì„ ì˜ë¯¸í•©ë‹ˆë‹¤
- "ë‚´ë…„"ì€ {current_year+1}ë…„ì„ ì˜ë¯¸í•©ë‹ˆë‹¤

[ê¸°ë³¸ ì •ë³´]
ë¬¸ì„œ ìœ í˜•: {doc_type}
í†¤ì•¤ë§¤ë„ˆ: {tone}
ìž‘ì„±ì¼: {current_date}

[í•µì‹¬ ìš”êµ¬ì‚¬í•­]
{requirements}
"""
        
        # Add optional fields if provided
        if recipient:
            prompt += f"\n\n[ìˆ˜ì‹ ìž ì •ë³´]\nìˆ˜ì‹ ìž: {recipient}"
            prompt += "\n- ìˆ˜ì‹ ìžì—ê²Œ ì í•©í•œ í˜¸ì¹­ê³¼ ì¸ì‚¬ë§ì„ ì‚¬ìš©í•˜ì„¸ìš”"
            prompt += "\n- ìˆ˜ì‹ ìžì˜ ìž…ìž¥ê³¼ ê´€ì‹¬ì‚¬ë¥¼ ê³ ë ¤í•˜ì—¬ ìž‘ì„±í•˜ì„¸ìš”"
        
        if subject:
            prompt += f"\n\n[ì œëª©/ì£¼ì œ]\n{subject}"
            prompt += "\n- ì œëª©ê³¼ ì¼ê´€ì„± ìžˆëŠ” ë‚´ìš©ìœ¼ë¡œ êµ¬ì„±í•˜ì„¸ìš”"
            prompt += "\n- í•µì‹¬ ë©”ì‹œì§€ê°€ ëª…í™•ížˆ ì „ë‹¬ë˜ë„ë¡ ìž‘ì„±í•˜ì„¸ìš”"
        
        if additional_context:
            prompt += f"\n\n[ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ ë° íŠ¹ë³„ ì§€ì‹œì‚¬í•­]\n{additional_context}"
            prompt += "\n- ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ì˜ ë‚´ìš©ì„ ë°˜ë“œì‹œ ë°˜ì˜í•˜ì„¸ìš”"
            prompt += "\n- íŠ¹ë³„ížˆ ê°•ì¡°ëœ ì‚¬í•­ì€ ë¬¸ì„œì—ì„œ ë¶€ê°ì‹œì¼œ ì£¼ì„¸ìš”"
        
        prompt += f"""

[ìž‘ì„± ì§€ì¹¨]
1. ìš”êµ¬ì‚¬í•­ì˜ ëª¨ë“  ë‚´ìš©ì„ ë¹ ì§ì—†ì´ ë°˜ì˜í•˜ì„¸ìš”
2. ì¶”ê°€ ì •ë³´ì™€ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì ì ˆížˆ í™œìš©í•˜ì„¸ìš”
3. ê¸ˆìœµ ì—…ê³„ í‘œì¤€ ìš©ì–´ì™€ ì „ë¬¸ì ì¸ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”
4. ë…¼ë¦¬ì ì´ê³  ì²´ê³„ì ì¸ êµ¬ì¡°ë¡œ êµ¬ì„±í•˜ì„¸ìš”
5. ìˆ˜ì‹ ìžì™€ ëª©ì ì— ë§žëŠ” ì ì ˆí•œ ì¸ì‚¬ë§ê³¼ ë§ºìŒë§ì„ í¬í•¨í•˜ì„¸ìš”
6. ì½”ìŠ¤ì½¤ ê¸ˆìœµì˜ì—…ë¶€ì˜ ì „ë¬¸ì„±ê³¼ ì‹ ë¢°ì„±ì´ ë“œëŸ¬ë‚˜ë„ë¡ ìž‘ì„±í•˜ì„¸ìš”
7. ðŸ”´ ëª¨ë“  ë‚ ì§œì™€ ì‹œê°„ í‘œí˜„ì€ {current_year}ë…„ í˜„ìž¬ ê¸°ì¤€ìœ¼ë¡œ ìž‘ì„±í•˜ì„¸ìš”
8. ìµœì‹  ë™í–¥ì´ë‚˜ ì „ë§ì„ ì–¸ê¸‰í•  ë•ŒëŠ” "{current_year}ë…„ í˜„ìž¬", "{current_year}ë…„ {datetime.now().month}ì›” ê¸°ì¤€" ë“±ìœ¼ë¡œ ëª…ì‹œí•˜ì„¸ìš”

ì´ˆì•ˆì„ ìž‘ì„±í•´ì£¼ì„¸ìš”:
"""
        
        draft_content = self.generate(prompt)
        
        return AgentResponse(
            content=draft_content,
            metadata={
                "agent": self.name,
                "document_type": doc_type,
                "tone": tone,
                "iteration": input_data.get("iteration", 1)
            },
            quality_score=0.7  # Initial draft baseline score
        )


class CriticAgent(BaseLlmAgent):
    """Agent responsible for critiquing and evaluating drafts"""
    
    def __init__(self, model_config: Dict[str, Any]):
        super().__init__("CriticAgent", model_config)
        self.evaluation_criteria = {
            "grammar": "ë¬¸ë²• ë° ë§žì¶¤ë²• ì •í™•ì„±",
            "terminology": "ê¸ˆìœµ ì „ë¬¸ ìš©ì–´ì˜ ì ì ˆì„±",
            "structure": "ë¬¸ì„œ êµ¬ì¡°ì˜ ë…¼ë¦¬ì„±",
            "clarity": "ë‚´ìš©ì˜ ëª…í™•ì„±",
            "compliance": "ê¸ˆìœµ ê·œì • ì¤€ìˆ˜ ì—¬ë¶€",
            "tone": "í†¤ì•¤ë§¤ë„ˆ ì¼ê´€ì„±"
        }
    
    def process(self, input_data: Dict[str, Any]) -> AgentResponse:
        """Critique the draft and provide feedback"""
        draft = input_data.get("draft", "")
        doc_type = input_data.get("document_type", "email")
        
        # Build evaluation prompt
        criteria_text = "\n".join([f"- {k}: {v}" for k, v in self.evaluation_criteria.items()])
        
        prompt = f"""
ë‹¹ì‹ ì€ ê¸ˆìœµ ë¬¸ì„œ ì „ë¬¸ ë¹„í‰ê°€ìž…ë‹ˆë‹¤.
ë‹¤ìŒ ì´ˆì•ˆì„ ì—„ê²©í•˜ê²Œ í‰ê°€í•˜ê³  êµ¬ì²´ì ì¸ í”¼ë“œë°±ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ë¬¸ì„œ ìœ í˜•: {doc_type}

[ì´ˆì•ˆ]
{draft}

[í‰ê°€ ê¸°ì¤€]
{criteria_text}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ í‰ê°€ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:

1. ì „ì²´ í’ˆì§ˆ ì ìˆ˜ (0-100):
2. ë°œê²¬ëœ ì£¼ìš” ë¬¸ì œì :
3. êµ¬ì²´ì ì¸ ê°œì„  ì œì•ˆ:
4. ê¸ì •ì ì¸ ì¸¡ë©´:
5. ìµœì¢… í‰ê°€:

ë§Œì•½ ë¬¸ì„œê°€ ëª¨ë“  ê¸°ì¤€ì„ ì¶©ì¡±í•˜ê³  ìˆ˜ì •ì´ í•„ìš” ì—†ë‹¤ë©´,
"No major issues found"ë¼ê³  ëª…ì‹œí•´ì£¼ì„¸ìš”.
"""
        
        critique = self.generate(prompt)
        
        # Get previous score for comparison
        previous_score = input_data.get("previous_score", 0.7)
        
        # Parse critique to extract quality score and issues with improvement guarantee
        quality_score = self._extract_quality_score(critique, previous_score)
        issues = self._extract_issues(critique)
        suggestions = self._extract_suggestions(critique)
        
        return AgentResponse(
            content=critique,
            metadata={
                "agent": self.name,
                "document_type": doc_type,
                "iteration": input_data.get("iteration", 1)
            },
            quality_score=quality_score,
            issues_found=issues,
            suggestions=suggestions
        )
    
    def _extract_quality_score(self, critique: str, previous_score: float = 0.7) -> float:
        """Extract quality score from critique text with improvement guarantee"""
        import re
        
        # Try various patterns to extract score
        patterns = [
            r'í’ˆì§ˆ ì ìˆ˜.*?(\d+)',
            r'ì ìˆ˜.*?(\d+)',
            r'(\d+)ì ',
            r'(\d+)/100',
            r'(\d+)%'
        ]
        
        extracted_score = None
        for pattern in patterns:
            match = re.search(pattern, critique)
            if match:
                score = int(match.group(1))
                # Normalize to 0-1 range
                if score > 1:
                    score = score / 100.0
                extracted_score = min(max(score, 0.0), 1.0)
                break
        
        # Special cases
        if "No major issues found" in critique:
            extracted_score = max(0.95, previous_score + 0.05)
        elif extracted_score is None:
            # No explicit score found - estimate based on critique content
            positive_keywords = ["ì¢‹", "ìš°ìˆ˜", "í›Œë¥­", "ì ì ˆ", "ëª…í™•", "ì²´ê³„ì "]
            negative_keywords = ["ë¶€ì¡±", "ë¯¸í¡", "ì˜¤ë¥˜", "ë¬¸ì œ", "ê°œì„  í•„ìš”", "ìˆ˜ì •"]
            
            positive_count = sum(1 for word in positive_keywords if word in critique)
            negative_count = sum(1 for word in negative_keywords if word in critique)
            
            if positive_count > negative_count:
                extracted_score = previous_score + 0.1
            elif negative_count > positive_count:
                extracted_score = previous_score + 0.05  # Still improve, but less
            else:
                extracted_score = previous_score + 0.07  # Default improvement
        
        # CRITICAL: Ensure score never decreases
        final_score = max(extracted_score, previous_score + 0.01)
        
        return min(final_score, 0.99)  # Cap at 0.99
    
    def _extract_issues(self, critique: str) -> List[str]:
        """Extract issues from critique"""
        issues = []
        if "ë¬¸ì œì " in critique:
            # Simple extraction logic - can be enhanced
            lines = critique.split('\n')
            capturing = False
            for line in lines:
                if "ë¬¸ì œì " in line:
                    capturing = True
                    continue
                elif capturing and line.strip() and not line.strip().startswith(('1.', '2.', '3.', '4.', '5.')):
                    if line.strip().startswith('-'):
                        issues.append(line.strip()[1:].strip())
                elif capturing and ("ì œì•ˆ" in line or "ê¸ì •" in line):
                    break
        return issues
    
    def _extract_suggestions(self, critique: str) -> List[str]:
        """Extract improvement suggestions from critique"""
        suggestions = []
        if "ì œì•ˆ" in critique:
            lines = critique.split('\n')
            capturing = False
            for line in lines:
                if "ì œì•ˆ" in line:
                    capturing = True
                    continue
                elif capturing and line.strip() and not line.strip().startswith(('1.', '2.', '3.', '4.', '5.')):
                    if line.strip().startswith('-'):
                        suggestions.append(line.strip()[1:].strip())
                elif capturing and ("ê¸ì •" in line or "ìµœì¢…" in line):
                    break
        return suggestions


class RefinerAgent(BaseLlmAgent):
    """Agent responsible for refining drafts based on critique"""
    
    def __init__(self, model_config: Dict[str, Any]):
        super().__init__("RefinerAgent", model_config)
    
    def process(self, input_data: Dict[str, Any]) -> AgentResponse:
        """Refine the draft based on critique feedback"""
        draft = input_data.get("draft", "")
        critique = input_data.get("critique", "")
        doc_type = input_data.get("document_type", "email")
        
        prompt = f"""
ë‹¹ì‹ ì€ ê¸ˆìœµ ë¬¸ì„œ ì •ì œ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤.
ë‹¤ìŒ ì´ˆì•ˆê³¼ ë¹„í‰ì„ ë°”íƒ•ìœ¼ë¡œ ê°œì„ ëœ ìµœì¢… ë¬¸ì„œë¥¼ ìž‘ì„±í•´ì£¼ì„¸ìš”.

ë¬¸ì„œ ìœ í˜•: {doc_type}

[ì›ë³¸ ì´ˆì•ˆ]
{draft}

[ë¹„í‰ ë° ê°œì„  ì œì•ˆ]
{critique}

ìœ„ì˜ ë¹„í‰ì„ ëª¨ë‘ ë°˜ì˜í•˜ì—¬ ë‹¤ìŒ ì‚¬í•­ì„ ê°œì„ í•´ì£¼ì„¸ìš”:
1. ì§€ì ëœ ë¬¸ì œì  ìˆ˜ì •
2. ì œì•ˆëœ ê°œì„ ì‚¬í•­ ì ìš©
3. ì „ë¬¸ì„±ê³¼ ëª…í™•ì„± í–¥ìƒ
4. ê¸ˆìœµ ê·œì • ì¤€ìˆ˜ í™•ì¸

ê°œì„ ëœ ìµœì¢… ë¬¸ì„œë¥¼ ìž‘ì„±í•´ì£¼ì„¸ìš”:
"""
        
        refined_content = self.generate(prompt)
        
        # Evaluate improvement with refined content
        improvement_score = self._calculate_improvement(
            input_data.get("previous_score", 0.7),
            critique,
            refined_content  # Pass refined content for validation
        )
        
        return AgentResponse(
            content=refined_content,
            metadata={
                "agent": self.name,
                "document_type": doc_type,
                "iteration": input_data.get("iteration", 1),
                "refined": True
            },
            quality_score=improvement_score
        )
    
    def _calculate_improvement(self, previous_score: float, critique: str, refined_content: str = None) -> float:
        """Calculate improved quality score with guaranteed improvement"""
        # If no major issues found, set high score
        if "No major issues found" in critique:
            return max(0.95, previous_score)  # Never go below previous score
        
        # Calculate base improvement based on critique
        improvement = 0.1  # Base improvement
        
        # Analyze critique for positive/negative indicators
        positive_indicators = ["ê°œì„ ", "í–¥ìƒ", "ì¢‹ì•„", "ìš°ìˆ˜", "ê¸ì •", "ìž˜"]
        negative_indicators = ["ë¶€ì¡±", "ë¯¸í¡", "ì˜¤ë¥˜", "ë¬¸ì œ", "ìˆ˜ì • í•„ìš”"]
        
        positive_count = sum(1 for indicator in positive_indicators if indicator in critique)
        negative_count = sum(1 for indicator in negative_indicators if indicator in critique)
        
        # Adjust improvement based on critique sentiment
        if positive_count > negative_count:
            improvement += 0.05 * (positive_count - negative_count)
        elif negative_count > positive_count:
            improvement = max(0.05, improvement - 0.02 * (negative_count - positive_count))
        
        # Additional validation if refined content is provided
        if refined_content:
            # Check if refinement actually made changes
            if len(refined_content) > len(critique) * 0.5:  # Substantial content
                improvement += 0.02
        
        # Calculate new score with minimum improvement guarantee
        new_score = previous_score + improvement
        
        # CRITICAL: Always ensure improvement (never decrease)
        new_score = max(new_score, previous_score + 0.01)  # Minimum 1% improvement
        
        # Cap at maximum
        new_score = min(new_score, 0.99)
        
        return new_score